{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mpl_toolkits\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAR = pd.read_csv(\"HAR_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V553</th>\n",
       "      <th>V554</th>\n",
       "      <th>V555</th>\n",
       "      <th>V556</th>\n",
       "      <th>V557</th>\n",
       "      <th>V558</th>\n",
       "      <th>V559</th>\n",
       "      <th>V560</th>\n",
       "      <th>V561</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989303</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "\n",
       "         V8        V9       V10  ...        V553      V554      V555  \\\n",
       "0 -0.983185 -0.923527 -0.934724  ...   -0.298676 -0.710304 -0.112754   \n",
       "1 -0.974914 -0.957686 -0.943068  ...   -0.595051 -0.861499  0.053477   \n",
       "2 -0.963668 -0.977469 -0.938692  ...   -0.390748 -0.760104 -0.118559   \n",
       "3 -0.982750 -0.989303 -0.938692  ...   -0.117290 -0.482845 -0.036788   \n",
       "4 -0.979672 -0.990441 -0.942469  ...   -0.351471 -0.699205  0.123320   \n",
       "\n",
       "       V556      V557      V558      V559      V560      V561  Class  \n",
       "0  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627      5  \n",
       "1 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317      5  \n",
       "2  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118      5  \n",
       "3 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663      5  \n",
       "4  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892      5  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 562)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1944\n",
       "5    1906\n",
       "4    1777\n",
       "1    1722\n",
       "2    1544\n",
       "3    1406\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAR['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1gwPSVJrhockqTXDQ5LUmuEhSWrt/wOCtT+PqF5gGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1062930b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106d45da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Class', data= HAR, palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = HAR['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        5\n",
       "2        5\n",
       "3        5\n",
       "4        5\n",
       "5        5\n",
       "6        5\n",
       "7        5\n",
       "8        5\n",
       "9        5\n",
       "10       5\n",
       "11       5\n",
       "12       5\n",
       "13       5\n",
       "14       5\n",
       "15       5\n",
       "16       5\n",
       "17       5\n",
       "18       5\n",
       "19       5\n",
       "20       5\n",
       "21       5\n",
       "22       5\n",
       "23       5\n",
       "24       5\n",
       "25       5\n",
       "26       5\n",
       "27       4\n",
       "28       4\n",
       "29       4\n",
       "        ..\n",
       "10269    2\n",
       "10270    2\n",
       "10271    2\n",
       "10272    2\n",
       "10273    2\n",
       "10274    2\n",
       "10275    2\n",
       "10276    2\n",
       "10277    2\n",
       "10278    2\n",
       "10279    2\n",
       "10280    3\n",
       "10281    3\n",
       "10282    3\n",
       "10283    3\n",
       "10284    3\n",
       "10285    3\n",
       "10286    3\n",
       "10287    3\n",
       "10288    3\n",
       "10289    2\n",
       "10290    2\n",
       "10291    2\n",
       "10292    2\n",
       "10293    2\n",
       "10294    2\n",
       "10295    2\n",
       "10296    2\n",
       "10297    2\n",
       "10298    2\n",
       "Name: Class, Length: 10299, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAR.drop('Class',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 561)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 561)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V552</th>\n",
       "      <th>V553</th>\n",
       "      <th>V554</th>\n",
       "      <th>V555</th>\n",
       "      <th>V556</th>\n",
       "      <th>V557</th>\n",
       "      <th>V558</th>\n",
       "      <th>V559</th>\n",
       "      <th>V560</th>\n",
       "      <th>V561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989303</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.277199</td>\n",
       "      <td>-0.010098</td>\n",
       "      <td>-0.105137</td>\n",
       "      <td>-0.997335</td>\n",
       "      <td>-0.990487</td>\n",
       "      <td>-0.995420</td>\n",
       "      <td>-0.997627</td>\n",
       "      <td>-0.990218</td>\n",
       "      <td>-0.995549</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019953</td>\n",
       "      <td>-0.545410</td>\n",
       "      <td>-0.844619</td>\n",
       "      <td>0.082632</td>\n",
       "      <td>-0.143439</td>\n",
       "      <td>0.275041</td>\n",
       "      <td>-0.368224</td>\n",
       "      <td>-0.849632</td>\n",
       "      <td>0.184823</td>\n",
       "      <td>-0.042126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.279454</td>\n",
       "      <td>-0.019641</td>\n",
       "      <td>-0.110022</td>\n",
       "      <td>-0.996921</td>\n",
       "      <td>-0.967186</td>\n",
       "      <td>-0.983118</td>\n",
       "      <td>-0.997003</td>\n",
       "      <td>-0.966097</td>\n",
       "      <td>-0.983116</td>\n",
       "      <td>-0.940987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>-0.217198</td>\n",
       "      <td>-0.564430</td>\n",
       "      <td>-0.212754</td>\n",
       "      <td>-0.230622</td>\n",
       "      <td>0.014637</td>\n",
       "      <td>-0.189512</td>\n",
       "      <td>-0.852150</td>\n",
       "      <td>0.182170</td>\n",
       "      <td>-0.043010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.277432</td>\n",
       "      <td>-0.030488</td>\n",
       "      <td>-0.125360</td>\n",
       "      <td>-0.996559</td>\n",
       "      <td>-0.966728</td>\n",
       "      <td>-0.981585</td>\n",
       "      <td>-0.996485</td>\n",
       "      <td>-0.966313</td>\n",
       "      <td>-0.982982</td>\n",
       "      <td>-0.940987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136382</td>\n",
       "      <td>-0.082307</td>\n",
       "      <td>-0.421715</td>\n",
       "      <td>-0.020888</td>\n",
       "      <td>0.593996</td>\n",
       "      <td>-0.561871</td>\n",
       "      <td>0.467383</td>\n",
       "      <td>-0.851017</td>\n",
       "      <td>0.183779</td>\n",
       "      <td>-0.041976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.277293</td>\n",
       "      <td>-0.021751</td>\n",
       "      <td>-0.120751</td>\n",
       "      <td>-0.997328</td>\n",
       "      <td>-0.961245</td>\n",
       "      <td>-0.983672</td>\n",
       "      <td>-0.997596</td>\n",
       "      <td>-0.957236</td>\n",
       "      <td>-0.984379</td>\n",
       "      <td>-0.940598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314038</td>\n",
       "      <td>-0.269401</td>\n",
       "      <td>-0.572995</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.080936</td>\n",
       "      <td>-0.234313</td>\n",
       "      <td>0.117797</td>\n",
       "      <td>-0.847971</td>\n",
       "      <td>0.188982</td>\n",
       "      <td>-0.037364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.280586</td>\n",
       "      <td>-0.009960</td>\n",
       "      <td>-0.106065</td>\n",
       "      <td>-0.994803</td>\n",
       "      <td>-0.972758</td>\n",
       "      <td>-0.986244</td>\n",
       "      <td>-0.995405</td>\n",
       "      <td>-0.973663</td>\n",
       "      <td>-0.985642</td>\n",
       "      <td>-0.940028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267383</td>\n",
       "      <td>0.339526</td>\n",
       "      <td>0.140452</td>\n",
       "      <td>-0.020590</td>\n",
       "      <td>-0.127730</td>\n",
       "      <td>-0.482871</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>-0.848294</td>\n",
       "      <td>0.190310</td>\n",
       "      <td>-0.034417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.276880</td>\n",
       "      <td>-0.012722</td>\n",
       "      <td>-0.103438</td>\n",
       "      <td>-0.994815</td>\n",
       "      <td>-0.973077</td>\n",
       "      <td>-0.985357</td>\n",
       "      <td>-0.995509</td>\n",
       "      <td>-0.973948</td>\n",
       "      <td>-0.985172</td>\n",
       "      <td>-0.940028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120503</td>\n",
       "      <td>0.348771</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.080699</td>\n",
       "      <td>0.595791</td>\n",
       "      <td>-0.475802</td>\n",
       "      <td>0.115931</td>\n",
       "      <td>-0.851562</td>\n",
       "      <td>0.187609</td>\n",
       "      <td>-0.034681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.276228</td>\n",
       "      <td>-0.021441</td>\n",
       "      <td>-0.108202</td>\n",
       "      <td>-0.998246</td>\n",
       "      <td>-0.987214</td>\n",
       "      <td>-0.992727</td>\n",
       "      <td>-0.998251</td>\n",
       "      <td>-0.985997</td>\n",
       "      <td>-0.993182</td>\n",
       "      <td>-0.943906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351442</td>\n",
       "      <td>-0.611014</td>\n",
       "      <td>-0.878363</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>-0.065980</td>\n",
       "      <td>0.578861</td>\n",
       "      <td>-0.651945</td>\n",
       "      <td>-0.852723</td>\n",
       "      <td>0.186050</td>\n",
       "      <td>-0.035852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.278457</td>\n",
       "      <td>-0.020415</td>\n",
       "      <td>-0.112732</td>\n",
       "      <td>-0.999135</td>\n",
       "      <td>-0.984680</td>\n",
       "      <td>-0.996274</td>\n",
       "      <td>-0.999077</td>\n",
       "      <td>-0.982937</td>\n",
       "      <td>-0.996410</td>\n",
       "      <td>-0.943906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689897</td>\n",
       "      <td>-0.686389</td>\n",
       "      <td>-0.878751</td>\n",
       "      <td>-0.077552</td>\n",
       "      <td>-0.101222</td>\n",
       "      <td>0.639084</td>\n",
       "      <td>0.765485</td>\n",
       "      <td>-0.850654</td>\n",
       "      <td>0.187611</td>\n",
       "      <td>-0.035998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.277175</td>\n",
       "      <td>-0.014713</td>\n",
       "      <td>-0.106756</td>\n",
       "      <td>-0.999188</td>\n",
       "      <td>-0.990526</td>\n",
       "      <td>-0.993365</td>\n",
       "      <td>-0.999211</td>\n",
       "      <td>-0.990687</td>\n",
       "      <td>-0.992168</td>\n",
       "      <td>-0.943323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740023</td>\n",
       "      <td>-0.564100</td>\n",
       "      <td>-0.765901</td>\n",
       "      <td>0.105620</td>\n",
       "      <td>-0.090278</td>\n",
       "      <td>-0.132403</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>-0.849773</td>\n",
       "      <td>0.188812</td>\n",
       "      <td>-0.035063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.297946</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>-0.061668</td>\n",
       "      <td>-0.988641</td>\n",
       "      <td>-0.816699</td>\n",
       "      <td>-0.901907</td>\n",
       "      <td>-0.988958</td>\n",
       "      <td>-0.794280</td>\n",
       "      <td>-0.888015</td>\n",
       "      <td>-0.925977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130958</td>\n",
       "      <td>0.207689</td>\n",
       "      <td>-0.068054</td>\n",
       "      <td>0.062297</td>\n",
       "      <td>-0.058719</td>\n",
       "      <td>0.031208</td>\n",
       "      <td>-0.268791</td>\n",
       "      <td>-0.730937</td>\n",
       "      <td>0.283159</td>\n",
       "      <td>0.036444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.279203</td>\n",
       "      <td>-0.023020</td>\n",
       "      <td>-0.122080</td>\n",
       "      <td>-0.996839</td>\n",
       "      <td>-0.974848</td>\n",
       "      <td>-0.983386</td>\n",
       "      <td>-0.997094</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-0.984065</td>\n",
       "      <td>-0.941716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661540</td>\n",
       "      <td>-0.782137</td>\n",
       "      <td>-0.953520</td>\n",
       "      <td>-0.121852</td>\n",
       "      <td>-0.029077</td>\n",
       "      <td>-0.013034</td>\n",
       "      <td>-0.056927</td>\n",
       "      <td>-0.761101</td>\n",
       "      <td>0.263119</td>\n",
       "      <td>0.024172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.279038</td>\n",
       "      <td>-0.014800</td>\n",
       "      <td>-0.116849</td>\n",
       "      <td>-0.996941</td>\n",
       "      <td>-0.981866</td>\n",
       "      <td>-0.982577</td>\n",
       "      <td>-0.997220</td>\n",
       "      <td>-0.981620</td>\n",
       "      <td>-0.981336</td>\n",
       "      <td>-0.941716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560668</td>\n",
       "      <td>-0.778877</td>\n",
       "      <td>-0.940421</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.048110</td>\n",
       "      <td>-0.340473</td>\n",
       "      <td>-0.229155</td>\n",
       "      <td>-0.759172</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0.027014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.280135</td>\n",
       "      <td>-0.013917</td>\n",
       "      <td>-0.106370</td>\n",
       "      <td>-0.997695</td>\n",
       "      <td>-0.987516</td>\n",
       "      <td>-0.990407</td>\n",
       "      <td>-0.998014</td>\n",
       "      <td>-0.987954</td>\n",
       "      <td>-0.992190</td>\n",
       "      <td>-0.942076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428614</td>\n",
       "      <td>-0.328899</td>\n",
       "      <td>-0.596861</td>\n",
       "      <td>-0.028332</td>\n",
       "      <td>0.092367</td>\n",
       "      <td>-0.822239</td>\n",
       "      <td>0.367557</td>\n",
       "      <td>-0.759363</td>\n",
       "      <td>0.264033</td>\n",
       "      <td>0.029664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.277731</td>\n",
       "      <td>-0.018211</td>\n",
       "      <td>-0.109188</td>\n",
       "      <td>-0.997491</td>\n",
       "      <td>-0.993222</td>\n",
       "      <td>-0.996128</td>\n",
       "      <td>-0.997903</td>\n",
       "      <td>-0.992711</td>\n",
       "      <td>-0.996492</td>\n",
       "      <td>-0.944870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348413</td>\n",
       "      <td>-0.501301</td>\n",
       "      <td>-0.838243</td>\n",
       "      <td>-0.165849</td>\n",
       "      <td>-0.033007</td>\n",
       "      <td>-0.240572</td>\n",
       "      <td>0.788193</td>\n",
       "      <td>-0.761052</td>\n",
       "      <td>0.262886</td>\n",
       "      <td>0.029346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.275568</td>\n",
       "      <td>-0.016980</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.997811</td>\n",
       "      <td>-0.990522</td>\n",
       "      <td>-0.997621</td>\n",
       "      <td>-0.998205</td>\n",
       "      <td>-0.989470</td>\n",
       "      <td>-0.997193</td>\n",
       "      <td>-0.945662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667156</td>\n",
       "      <td>-0.941671</td>\n",
       "      <td>-0.965522</td>\n",
       "      <td>0.244931</td>\n",
       "      <td>0.102569</td>\n",
       "      <td>0.066135</td>\n",
       "      <td>-0.411729</td>\n",
       "      <td>-0.760620</td>\n",
       "      <td>0.263169</td>\n",
       "      <td>0.029573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.277562</td>\n",
       "      <td>-0.014318</td>\n",
       "      <td>-0.107877</td>\n",
       "      <td>-0.997904</td>\n",
       "      <td>-0.994311</td>\n",
       "      <td>-0.995952</td>\n",
       "      <td>-0.998365</td>\n",
       "      <td>-0.993604</td>\n",
       "      <td>-0.995595</td>\n",
       "      <td>-0.941472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601134</td>\n",
       "      <td>-0.833890</td>\n",
       "      <td>-0.968580</td>\n",
       "      <td>0.160607</td>\n",
       "      <td>0.197774</td>\n",
       "      <td>0.257657</td>\n",
       "      <td>-0.381102</td>\n",
       "      <td>-0.760528</td>\n",
       "      <td>0.263183</td>\n",
       "      <td>0.030288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.277152</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>-0.106601</td>\n",
       "      <td>-0.997763</td>\n",
       "      <td>-0.989957</td>\n",
       "      <td>-0.996586</td>\n",
       "      <td>-0.998291</td>\n",
       "      <td>-0.989669</td>\n",
       "      <td>-0.996700</td>\n",
       "      <td>-0.941472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576339</td>\n",
       "      <td>-0.848198</td>\n",
       "      <td>-0.950247</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>-0.853711</td>\n",
       "      <td>-0.762023</td>\n",
       "      <td>0.262170</td>\n",
       "      <td>0.029987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.275676</td>\n",
       "      <td>-0.021264</td>\n",
       "      <td>-0.110801</td>\n",
       "      <td>-0.997862</td>\n",
       "      <td>-0.990091</td>\n",
       "      <td>-0.994593</td>\n",
       "      <td>-0.998333</td>\n",
       "      <td>-0.989473</td>\n",
       "      <td>-0.994485</td>\n",
       "      <td>-0.944567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480225</td>\n",
       "      <td>-0.701970</td>\n",
       "      <td>-0.895118</td>\n",
       "      <td>-0.032337</td>\n",
       "      <td>-0.301298</td>\n",
       "      <td>0.132576</td>\n",
       "      <td>-0.022379</td>\n",
       "      <td>-0.761509</td>\n",
       "      <td>0.262550</td>\n",
       "      <td>0.029639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.279200</td>\n",
       "      <td>-0.017714</td>\n",
       "      <td>-0.109161</td>\n",
       "      <td>-0.998389</td>\n",
       "      <td>-0.987308</td>\n",
       "      <td>-0.990832</td>\n",
       "      <td>-0.998869</td>\n",
       "      <td>-0.986771</td>\n",
       "      <td>-0.989637</td>\n",
       "      <td>-0.943675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468191</td>\n",
       "      <td>-0.909786</td>\n",
       "      <td>-0.986595</td>\n",
       "      <td>-0.542884</td>\n",
       "      <td>-0.249545</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>-0.235200</td>\n",
       "      <td>-0.758960</td>\n",
       "      <td>0.264256</td>\n",
       "      <td>0.030456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.281715</td>\n",
       "      <td>-0.011911</td>\n",
       "      <td>-0.102875</td>\n",
       "      <td>-0.998534</td>\n",
       "      <td>-0.988489</td>\n",
       "      <td>-0.993184</td>\n",
       "      <td>-0.998674</td>\n",
       "      <td>-0.988544</td>\n",
       "      <td>-0.993287</td>\n",
       "      <td>-0.942559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730084</td>\n",
       "      <td>-0.746382</td>\n",
       "      <td>-0.941664</td>\n",
       "      <td>-0.021446</td>\n",
       "      <td>0.337010</td>\n",
       "      <td>-0.436685</td>\n",
       "      <td>-0.622922</td>\n",
       "      <td>-0.758977</td>\n",
       "      <td>0.264224</td>\n",
       "      <td>0.030743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.278993</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.106596</td>\n",
       "      <td>-0.998060</td>\n",
       "      <td>-0.986070</td>\n",
       "      <td>-0.993424</td>\n",
       "      <td>-0.998059</td>\n",
       "      <td>-0.985192</td>\n",
       "      <td>-0.995018</td>\n",
       "      <td>-0.942559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677073</td>\n",
       "      <td>-0.715462</td>\n",
       "      <td>-0.937483</td>\n",
       "      <td>0.025652</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>-0.226316</td>\n",
       "      <td>-0.225358</td>\n",
       "      <td>-0.762197</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.029404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.275734</td>\n",
       "      <td>-0.018019</td>\n",
       "      <td>-0.106776</td>\n",
       "      <td>-0.999255</td>\n",
       "      <td>-0.993669</td>\n",
       "      <td>-0.994189</td>\n",
       "      <td>-0.999407</td>\n",
       "      <td>-0.993620</td>\n",
       "      <td>-0.993583</td>\n",
       "      <td>-0.942890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718284</td>\n",
       "      <td>-0.824919</td>\n",
       "      <td>-0.964407</td>\n",
       "      <td>0.231060</td>\n",
       "      <td>0.429283</td>\n",
       "      <td>0.681154</td>\n",
       "      <td>0.815226</td>\n",
       "      <td>-0.763702</td>\n",
       "      <td>0.261103</td>\n",
       "      <td>0.028563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.144504</td>\n",
       "      <td>0.189263</td>\n",
       "      <td>0.062769</td>\n",
       "      <td>-0.904300</td>\n",
       "      <td>-0.181937</td>\n",
       "      <td>-0.443151</td>\n",
       "      <td>-0.901100</td>\n",
       "      <td>-0.110813</td>\n",
       "      <td>-0.400599</td>\n",
       "      <td>-0.931896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257607</td>\n",
       "      <td>0.156185</td>\n",
       "      <td>-0.241781</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.043354</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.046689</td>\n",
       "      <td>-0.667085</td>\n",
       "      <td>0.054216</td>\n",
       "      <td>-0.218875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.287252</td>\n",
       "      <td>-0.037455</td>\n",
       "      <td>-0.145974</td>\n",
       "      <td>-0.982915</td>\n",
       "      <td>-0.891605</td>\n",
       "      <td>-0.941438</td>\n",
       "      <td>-0.984418</td>\n",
       "      <td>-0.891373</td>\n",
       "      <td>-0.933361</td>\n",
       "      <td>-0.931896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582426</td>\n",
       "      <td>-0.743397</td>\n",
       "      <td>-0.899523</td>\n",
       "      <td>0.194735</td>\n",
       "      <td>-0.148056</td>\n",
       "      <td>0.033529</td>\n",
       "      <td>-0.127028</td>\n",
       "      <td>-0.564807</td>\n",
       "      <td>-0.027045</td>\n",
       "      <td>-0.266055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.279998</td>\n",
       "      <td>-0.019484</td>\n",
       "      <td>-0.105724</td>\n",
       "      <td>-0.992818</td>\n",
       "      <td>-0.940350</td>\n",
       "      <td>-0.981493</td>\n",
       "      <td>-0.993092</td>\n",
       "      <td>-0.936927</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.935389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755593</td>\n",
       "      <td>-0.768623</td>\n",
       "      <td>-0.928548</td>\n",
       "      <td>-0.228688</td>\n",
       "      <td>-0.097215</td>\n",
       "      <td>0.024192</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>-0.579367</td>\n",
       "      <td>-0.021567</td>\n",
       "      <td>-0.257530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10269</th>\n",
       "      <td>0.376577</td>\n",
       "      <td>-0.018081</td>\n",
       "      <td>-0.109896</td>\n",
       "      <td>-0.314382</td>\n",
       "      <td>-0.152071</td>\n",
       "      <td>-0.213623</td>\n",
       "      <td>-0.393594</td>\n",
       "      <td>-0.180600</td>\n",
       "      <td>-0.265530</td>\n",
       "      <td>-0.072573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191368</td>\n",
       "      <td>-0.323285</td>\n",
       "      <td>-0.752765</td>\n",
       "      <td>-0.829318</td>\n",
       "      <td>0.048275</td>\n",
       "      <td>0.913474</td>\n",
       "      <td>-0.903792</td>\n",
       "      <td>-0.694649</td>\n",
       "      <td>0.245906</td>\n",
       "      <td>0.172715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>0.297434</td>\n",
       "      <td>-0.045270</td>\n",
       "      <td>-0.190596</td>\n",
       "      <td>-0.360521</td>\n",
       "      <td>-0.164580</td>\n",
       "      <td>-0.188636</td>\n",
       "      <td>-0.414821</td>\n",
       "      <td>-0.238855</td>\n",
       "      <td>-0.235164</td>\n",
       "      <td>-0.149166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043261</td>\n",
       "      <td>-0.221525</td>\n",
       "      <td>-0.650547</td>\n",
       "      <td>-0.241364</td>\n",
       "      <td>-0.181262</td>\n",
       "      <td>-0.946507</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>-0.722767</td>\n",
       "      <td>0.244779</td>\n",
       "      <td>0.144695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>0.253323</td>\n",
       "      <td>-0.024865</td>\n",
       "      <td>-0.170146</td>\n",
       "      <td>-0.307713</td>\n",
       "      <td>-0.188579</td>\n",
       "      <td>-0.141310</td>\n",
       "      <td>-0.377426</td>\n",
       "      <td>-0.226048</td>\n",
       "      <td>-0.220538</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178355</td>\n",
       "      <td>-0.141846</td>\n",
       "      <td>-0.564142</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.357282</td>\n",
       "      <td>-0.945578</td>\n",
       "      <td>0.613976</td>\n",
       "      <td>-0.694613</td>\n",
       "      <td>0.259140</td>\n",
       "      <td>0.157683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10272</th>\n",
       "      <td>0.208102</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>-0.075870</td>\n",
       "      <td>-0.323045</td>\n",
       "      <td>-0.095884</td>\n",
       "      <td>-0.217678</td>\n",
       "      <td>-0.424824</td>\n",
       "      <td>-0.130102</td>\n",
       "      <td>-0.322741</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119239</td>\n",
       "      <td>-0.297224</td>\n",
       "      <td>-0.721821</td>\n",
       "      <td>0.752448</td>\n",
       "      <td>-0.835923</td>\n",
       "      <td>-0.918626</td>\n",
       "      <td>0.109159</td>\n",
       "      <td>-0.671194</td>\n",
       "      <td>0.265751</td>\n",
       "      <td>0.174766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>0.144149</td>\n",
       "      <td>-0.039858</td>\n",
       "      <td>-0.045135</td>\n",
       "      <td>-0.354687</td>\n",
       "      <td>-0.015797</td>\n",
       "      <td>-0.233415</td>\n",
       "      <td>-0.465077</td>\n",
       "      <td>-0.036371</td>\n",
       "      <td>-0.327340</td>\n",
       "      <td>-0.017856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143979</td>\n",
       "      <td>-0.374529</td>\n",
       "      <td>-0.779528</td>\n",
       "      <td>0.556469</td>\n",
       "      <td>-0.206396</td>\n",
       "      <td>-0.943786</td>\n",
       "      <td>0.297510</td>\n",
       "      <td>-0.661045</td>\n",
       "      <td>0.270618</td>\n",
       "      <td>0.179726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10274</th>\n",
       "      <td>0.230852</td>\n",
       "      <td>-0.042286</td>\n",
       "      <td>-0.089920</td>\n",
       "      <td>-0.309347</td>\n",
       "      <td>-0.079126</td>\n",
       "      <td>-0.151727</td>\n",
       "      <td>-0.391110</td>\n",
       "      <td>-0.087048</td>\n",
       "      <td>-0.256664</td>\n",
       "      <td>0.056244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>-0.138929</td>\n",
       "      <td>-0.589311</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.855750</td>\n",
       "      <td>-0.962498</td>\n",
       "      <td>0.953145</td>\n",
       "      <td>-0.657085</td>\n",
       "      <td>0.276184</td>\n",
       "      <td>0.177337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10275</th>\n",
       "      <td>0.296221</td>\n",
       "      <td>-0.052292</td>\n",
       "      <td>-0.115492</td>\n",
       "      <td>-0.284000</td>\n",
       "      <td>-0.110110</td>\n",
       "      <td>-0.234592</td>\n",
       "      <td>-0.359815</td>\n",
       "      <td>-0.136375</td>\n",
       "      <td>-0.290923</td>\n",
       "      <td>0.056244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>-0.291001</td>\n",
       "      <td>-0.704020</td>\n",
       "      <td>-0.300646</td>\n",
       "      <td>0.225721</td>\n",
       "      <td>0.868469</td>\n",
       "      <td>-0.461542</td>\n",
       "      <td>-0.663506</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.173953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10276</th>\n",
       "      <td>0.357298</td>\n",
       "      <td>-0.044599</td>\n",
       "      <td>-0.129532</td>\n",
       "      <td>-0.314497</td>\n",
       "      <td>-0.055580</td>\n",
       "      <td>-0.173090</td>\n",
       "      <td>-0.385964</td>\n",
       "      <td>-0.057525</td>\n",
       "      <td>-0.216827</td>\n",
       "      <td>0.026249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.162555</td>\n",
       "      <td>-0.593285</td>\n",
       "      <td>-0.710861</td>\n",
       "      <td>-0.061232</td>\n",
       "      <td>-0.706116</td>\n",
       "      <td>0.064574</td>\n",
       "      <td>-0.660163</td>\n",
       "      <td>0.274327</td>\n",
       "      <td>0.176291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10277</th>\n",
       "      <td>0.344328</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>-0.122453</td>\n",
       "      <td>-0.319987</td>\n",
       "      <td>-0.066677</td>\n",
       "      <td>-0.181803</td>\n",
       "      <td>-0.380404</td>\n",
       "      <td>-0.071025</td>\n",
       "      <td>-0.244559</td>\n",
       "      <td>-0.116867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029216</td>\n",
       "      <td>0.181042</td>\n",
       "      <td>-0.250170</td>\n",
       "      <td>-0.402779</td>\n",
       "      <td>-0.706228</td>\n",
       "      <td>0.738571</td>\n",
       "      <td>0.870613</td>\n",
       "      <td>-0.652636</td>\n",
       "      <td>0.277868</td>\n",
       "      <td>0.180047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10278</th>\n",
       "      <td>0.283644</td>\n",
       "      <td>-0.007958</td>\n",
       "      <td>-0.119018</td>\n",
       "      <td>-0.308558</td>\n",
       "      <td>-0.080392</td>\n",
       "      <td>-0.211476</td>\n",
       "      <td>-0.368919</td>\n",
       "      <td>-0.097063</td>\n",
       "      <td>-0.301231</td>\n",
       "      <td>-0.116867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110489</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>-0.392944</td>\n",
       "      <td>-0.076085</td>\n",
       "      <td>-0.238582</td>\n",
       "      <td>0.960357</td>\n",
       "      <td>0.086598</td>\n",
       "      <td>-0.656760</td>\n",
       "      <td>0.271648</td>\n",
       "      <td>0.182987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10279</th>\n",
       "      <td>0.206988</td>\n",
       "      <td>0.024602</td>\n",
       "      <td>-0.103940</td>\n",
       "      <td>-0.364650</td>\n",
       "      <td>-0.169378</td>\n",
       "      <td>-0.215814</td>\n",
       "      <td>-0.449295</td>\n",
       "      <td>-0.185957</td>\n",
       "      <td>-0.326199</td>\n",
       "      <td>-0.175652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214243</td>\n",
       "      <td>-0.351854</td>\n",
       "      <td>-0.734494</td>\n",
       "      <td>0.535018</td>\n",
       "      <td>-0.256868</td>\n",
       "      <td>0.927325</td>\n",
       "      <td>-0.084328</td>\n",
       "      <td>-0.657011</td>\n",
       "      <td>0.266990</td>\n",
       "      <td>0.187901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10280</th>\n",
       "      <td>0.392804</td>\n",
       "      <td>-0.017788</td>\n",
       "      <td>-0.090166</td>\n",
       "      <td>-0.096349</td>\n",
       "      <td>-0.174368</td>\n",
       "      <td>-0.256686</td>\n",
       "      <td>-0.152987</td>\n",
       "      <td>-0.207809</td>\n",
       "      <td>-0.265253</td>\n",
       "      <td>0.485680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089373</td>\n",
       "      <td>0.274132</td>\n",
       "      <td>-0.036768</td>\n",
       "      <td>-0.742630</td>\n",
       "      <td>-0.080227</td>\n",
       "      <td>0.927331</td>\n",
       "      <td>-0.652179</td>\n",
       "      <td>-0.807271</td>\n",
       "      <td>0.189885</td>\n",
       "      <td>0.118456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10281</th>\n",
       "      <td>0.301162</td>\n",
       "      <td>-0.030034</td>\n",
       "      <td>-0.120333</td>\n",
       "      <td>-0.056181</td>\n",
       "      <td>-0.151753</td>\n",
       "      <td>-0.254848</td>\n",
       "      <td>-0.093334</td>\n",
       "      <td>-0.206796</td>\n",
       "      <td>-0.275332</td>\n",
       "      <td>0.485680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074873</td>\n",
       "      <td>0.430681</td>\n",
       "      <td>0.204816</td>\n",
       "      <td>-0.464962</td>\n",
       "      <td>0.291873</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>-0.869951</td>\n",
       "      <td>-0.814760</td>\n",
       "      <td>0.190418</td>\n",
       "      <td>0.110325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10282</th>\n",
       "      <td>0.288215</td>\n",
       "      <td>0.011493</td>\n",
       "      <td>-0.155453</td>\n",
       "      <td>-0.063550</td>\n",
       "      <td>-0.035160</td>\n",
       "      <td>-0.230059</td>\n",
       "      <td>-0.093119</td>\n",
       "      <td>-0.116987</td>\n",
       "      <td>-0.258696</td>\n",
       "      <td>0.160374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112290</td>\n",
       "      <td>0.055902</td>\n",
       "      <td>-0.299539</td>\n",
       "      <td>-0.057336</td>\n",
       "      <td>-0.592802</td>\n",
       "      <td>0.964828</td>\n",
       "      <td>0.606231</td>\n",
       "      <td>-0.809277</td>\n",
       "      <td>0.192232</td>\n",
       "      <td>0.113997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10283</th>\n",
       "      <td>0.114866</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>-0.116904</td>\n",
       "      <td>-0.189544</td>\n",
       "      <td>0.078012</td>\n",
       "      <td>-0.312489</td>\n",
       "      <td>-0.268158</td>\n",
       "      <td>-0.047632</td>\n",
       "      <td>-0.341972</td>\n",
       "      <td>0.160374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120152</td>\n",
       "      <td>-0.148767</td>\n",
       "      <td>-0.564841</td>\n",
       "      <td>0.849907</td>\n",
       "      <td>-0.643277</td>\n",
       "      <td>0.973227</td>\n",
       "      <td>0.062755</td>\n",
       "      <td>-0.809257</td>\n",
       "      <td>0.185954</td>\n",
       "      <td>0.120358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10284</th>\n",
       "      <td>0.126970</td>\n",
       "      <td>-0.013276</td>\n",
       "      <td>-0.073441</td>\n",
       "      <td>-0.173816</td>\n",
       "      <td>0.041540</td>\n",
       "      <td>-0.342513</td>\n",
       "      <td>-0.253156</td>\n",
       "      <td>-0.093578</td>\n",
       "      <td>-0.350155</td>\n",
       "      <td>0.285773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199365</td>\n",
       "      <td>0.048103</td>\n",
       "      <td>-0.252670</td>\n",
       "      <td>0.872466</td>\n",
       "      <td>0.491872</td>\n",
       "      <td>-0.155724</td>\n",
       "      <td>-0.762511</td>\n",
       "      <td>-0.800932</td>\n",
       "      <td>0.190968</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10285</th>\n",
       "      <td>0.363943</td>\n",
       "      <td>-0.022212</td>\n",
       "      <td>-0.125437</td>\n",
       "      <td>-0.041180</td>\n",
       "      <td>0.020777</td>\n",
       "      <td>-0.243607</td>\n",
       "      <td>-0.113519</td>\n",
       "      <td>-0.120400</td>\n",
       "      <td>-0.221709</td>\n",
       "      <td>0.411506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155538</td>\n",
       "      <td>0.118344</td>\n",
       "      <td>-0.205856</td>\n",
       "      <td>-0.895624</td>\n",
       "      <td>0.648047</td>\n",
       "      <td>-0.624180</td>\n",
       "      <td>-0.441569</td>\n",
       "      <td>-0.803888</td>\n",
       "      <td>0.191732</td>\n",
       "      <td>0.119944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>0.331044</td>\n",
       "      <td>-0.063979</td>\n",
       "      <td>-0.117333</td>\n",
       "      <td>-0.068002</td>\n",
       "      <td>0.156431</td>\n",
       "      <td>-0.317060</td>\n",
       "      <td>-0.148922</td>\n",
       "      <td>0.070107</td>\n",
       "      <td>-0.290868</td>\n",
       "      <td>0.411506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021385</td>\n",
       "      <td>-0.086279</td>\n",
       "      <td>-0.468001</td>\n",
       "      <td>-0.351287</td>\n",
       "      <td>-0.335934</td>\n",
       "      <td>0.966914</td>\n",
       "      <td>-0.715323</td>\n",
       "      <td>-0.810091</td>\n",
       "      <td>0.184712</td>\n",
       "      <td>0.120724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10287</th>\n",
       "      <td>0.272259</td>\n",
       "      <td>-0.007579</td>\n",
       "      <td>-0.072642</td>\n",
       "      <td>-0.072559</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>-0.225400</td>\n",
       "      <td>-0.161118</td>\n",
       "      <td>-0.074187</td>\n",
       "      <td>-0.280956</td>\n",
       "      <td>0.402615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026374</td>\n",
       "      <td>-0.104077</td>\n",
       "      <td>-0.408741</td>\n",
       "      <td>0.181631</td>\n",
       "      <td>0.491898</td>\n",
       "      <td>-0.977969</td>\n",
       "      <td>-0.124243</td>\n",
       "      <td>-0.797719</td>\n",
       "      <td>0.194819</td>\n",
       "      <td>0.122937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10288</th>\n",
       "      <td>0.277271</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-0.068523</td>\n",
       "      <td>-0.245059</td>\n",
       "      <td>-0.144552</td>\n",
       "      <td>-0.148731</td>\n",
       "      <td>-0.303187</td>\n",
       "      <td>-0.198912</td>\n",
       "      <td>0.402615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103873</td>\n",
       "      <td>0.160861</td>\n",
       "      <td>-0.125531</td>\n",
       "      <td>0.133749</td>\n",
       "      <td>0.882511</td>\n",
       "      <td>-0.994293</td>\n",
       "      <td>0.475236</td>\n",
       "      <td>-0.803914</td>\n",
       "      <td>0.196823</td>\n",
       "      <td>0.114498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10289</th>\n",
       "      <td>0.305631</td>\n",
       "      <td>-0.052702</td>\n",
       "      <td>-0.155684</td>\n",
       "      <td>-0.343078</td>\n",
       "      <td>-0.151250</td>\n",
       "      <td>-0.177296</td>\n",
       "      <td>-0.383228</td>\n",
       "      <td>-0.181989</td>\n",
       "      <td>-0.287325</td>\n",
       "      <td>-0.108216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032052</td>\n",
       "      <td>-0.285985</td>\n",
       "      <td>-0.714939</td>\n",
       "      <td>-0.338596</td>\n",
       "      <td>0.363565</td>\n",
       "      <td>-0.951163</td>\n",
       "      <td>-0.228412</td>\n",
       "      <td>-0.691122</td>\n",
       "      <td>0.241953</td>\n",
       "      <td>0.180141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10290</th>\n",
       "      <td>0.321443</td>\n",
       "      <td>-0.023799</td>\n",
       "      <td>-0.119695</td>\n",
       "      <td>-0.336548</td>\n",
       "      <td>-0.183307</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.396342</td>\n",
       "      <td>-0.224409</td>\n",
       "      <td>-0.220786</td>\n",
       "      <td>-0.073510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>-0.172403</td>\n",
       "      <td>-0.563389</td>\n",
       "      <td>-0.874477</td>\n",
       "      <td>-0.684506</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>0.472612</td>\n",
       "      <td>-0.677946</td>\n",
       "      <td>0.256877</td>\n",
       "      <td>0.177768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10291</th>\n",
       "      <td>0.267413</td>\n",
       "      <td>-0.021596</td>\n",
       "      <td>-0.070550</td>\n",
       "      <td>-0.323426</td>\n",
       "      <td>-0.118042</td>\n",
       "      <td>-0.323907</td>\n",
       "      <td>-0.395468</td>\n",
       "      <td>-0.129623</td>\n",
       "      <td>-0.335741</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215341</td>\n",
       "      <td>-0.388926</td>\n",
       "      <td>-0.761280</td>\n",
       "      <td>0.218079</td>\n",
       "      <td>-0.690839</td>\n",
       "      <td>-0.922779</td>\n",
       "      <td>0.232523</td>\n",
       "      <td>-0.672635</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.178609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10292</th>\n",
       "      <td>0.147146</td>\n",
       "      <td>-0.046908</td>\n",
       "      <td>-0.069338</td>\n",
       "      <td>-0.348686</td>\n",
       "      <td>0.018057</td>\n",
       "      <td>-0.364716</td>\n",
       "      <td>-0.437199</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>-0.411240</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166456</td>\n",
       "      <td>-0.552567</td>\n",
       "      <td>-0.850109</td>\n",
       "      <td>0.524082</td>\n",
       "      <td>0.041970</td>\n",
       "      <td>-0.922941</td>\n",
       "      <td>0.489178</td>\n",
       "      <td>-0.660366</td>\n",
       "      <td>0.272243</td>\n",
       "      <td>0.178547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10293</th>\n",
       "      <td>0.192275</td>\n",
       "      <td>-0.033643</td>\n",
       "      <td>-0.105949</td>\n",
       "      <td>-0.354841</td>\n",
       "      <td>-0.092504</td>\n",
       "      <td>-0.312910</td>\n",
       "      <td>-0.433579</td>\n",
       "      <td>-0.088741</td>\n",
       "      <td>-0.336060</td>\n",
       "      <td>-0.041624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158728</td>\n",
       "      <td>-0.629657</td>\n",
       "      <td>-0.916493</td>\n",
       "      <td>0.535983</td>\n",
       "      <td>0.689306</td>\n",
       "      <td>-0.936606</td>\n",
       "      <td>0.562375</td>\n",
       "      <td>-0.646754</td>\n",
       "      <td>0.282150</td>\n",
       "      <td>0.181152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>0.310155</td>\n",
       "      <td>-0.053391</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>-0.140589</td>\n",
       "      <td>-0.215088</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>-0.148775</td>\n",
       "      <td>-0.232057</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074472</td>\n",
       "      <td>-0.376278</td>\n",
       "      <td>-0.750809</td>\n",
       "      <td>-0.337422</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>-0.698885</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10295</th>\n",
       "      <td>0.363385</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>-0.105915</td>\n",
       "      <td>-0.305388</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>-0.196373</td>\n",
       "      <td>-0.373540</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.270237</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101859</td>\n",
       "      <td>-0.320418</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>-0.736701</td>\n",
       "      <td>-0.372889</td>\n",
       "      <td>-0.657421</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>-0.655181</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.182412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>0.349966</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>-0.329638</td>\n",
       "      <td>-0.042143</td>\n",
       "      <td>-0.250181</td>\n",
       "      <td>-0.388017</td>\n",
       "      <td>-0.133257</td>\n",
       "      <td>-0.347029</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066249</td>\n",
       "      <td>-0.118854</td>\n",
       "      <td>-0.467179</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.696664</td>\n",
       "      <td>0.363139</td>\n",
       "      <td>-0.655357</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.181184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.096499</td>\n",
       "      <td>-0.323114</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>-0.289477</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046467</td>\n",
       "      <td>-0.205445</td>\n",
       "      <td>-0.617737</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.819188</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.659719</td>\n",
       "      <td>0.264782</td>\n",
       "      <td>0.187563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>0.153627</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>-0.137018</td>\n",
       "      <td>-0.330046</td>\n",
       "      <td>-0.195253</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-0.430974</td>\n",
       "      <td>-0.218295</td>\n",
       "      <td>-0.229933</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010386</td>\n",
       "      <td>-0.072237</td>\n",
       "      <td>-0.436940</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>-0.287951</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>-0.660080</td>\n",
       "      <td>0.263936</td>\n",
       "      <td>0.188103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10299 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1      0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2      0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3      0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4      0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "5      0.277199 -0.010098 -0.105137 -0.997335 -0.990487 -0.995420 -0.997627   \n",
       "6      0.279454 -0.019641 -0.110022 -0.996921 -0.967186 -0.983118 -0.997003   \n",
       "7      0.277432 -0.030488 -0.125360 -0.996559 -0.966728 -0.981585 -0.996485   \n",
       "8      0.277293 -0.021751 -0.120751 -0.997328 -0.961245 -0.983672 -0.997596   \n",
       "9      0.280586 -0.009960 -0.106065 -0.994803 -0.972758 -0.986244 -0.995405   \n",
       "10     0.276880 -0.012722 -0.103438 -0.994815 -0.973077 -0.985357 -0.995509   \n",
       "11     0.276228 -0.021441 -0.108202 -0.998246 -0.987214 -0.992727 -0.998251   \n",
       "12     0.278457 -0.020415 -0.112732 -0.999135 -0.984680 -0.996274 -0.999077   \n",
       "13     0.277175 -0.014713 -0.106756 -0.999188 -0.990526 -0.993365 -0.999211   \n",
       "14     0.297946  0.027094 -0.061668 -0.988641 -0.816699 -0.901907 -0.988958   \n",
       "15     0.279203 -0.023020 -0.122080 -0.996839 -0.974848 -0.983386 -0.997094   \n",
       "16     0.279038 -0.014800 -0.116849 -0.996941 -0.981866 -0.982577 -0.997220   \n",
       "17     0.280135 -0.013917 -0.106370 -0.997695 -0.987516 -0.990407 -0.998014   \n",
       "18     0.277731 -0.018211 -0.109188 -0.997491 -0.993222 -0.996128 -0.997903   \n",
       "19     0.275568 -0.016980 -0.111429 -0.997811 -0.990522 -0.997621 -0.998205   \n",
       "20     0.277562 -0.014318 -0.107877 -0.997904 -0.994311 -0.995952 -0.998365   \n",
       "21     0.277152 -0.017983 -0.106601 -0.997763 -0.989957 -0.996586 -0.998291   \n",
       "22     0.275676 -0.021264 -0.110801 -0.997862 -0.990091 -0.994593 -0.998333   \n",
       "23     0.279200 -0.017714 -0.109161 -0.998389 -0.987308 -0.990832 -0.998869   \n",
       "24     0.281715 -0.011911 -0.102875 -0.998534 -0.988489 -0.993184 -0.998674   \n",
       "25     0.278993 -0.014531 -0.106596 -0.998060 -0.986070 -0.993424 -0.998059   \n",
       "26     0.275734 -0.018019 -0.106776 -0.999255 -0.993669 -0.994189 -0.999407   \n",
       "27     0.144504  0.189263  0.062769 -0.904300 -0.181937 -0.443151 -0.901100   \n",
       "28     0.287252 -0.037455 -0.145974 -0.982915 -0.891605 -0.941438 -0.984418   \n",
       "29     0.279998 -0.019484 -0.105724 -0.992818 -0.940350 -0.981493 -0.993092   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10269  0.376577 -0.018081 -0.109896 -0.314382 -0.152071 -0.213623 -0.393594   \n",
       "10270  0.297434 -0.045270 -0.190596 -0.360521 -0.164580 -0.188636 -0.414821   \n",
       "10271  0.253323 -0.024865 -0.170146 -0.307713 -0.188579 -0.141310 -0.377426   \n",
       "10272  0.208102 -0.008046 -0.075870 -0.323045 -0.095884 -0.217678 -0.424824   \n",
       "10273  0.144149 -0.039858 -0.045135 -0.354687 -0.015797 -0.233415 -0.465077   \n",
       "10274  0.230852 -0.042286 -0.089920 -0.309347 -0.079126 -0.151727 -0.391110   \n",
       "10275  0.296221 -0.052292 -0.115492 -0.284000 -0.110110 -0.234592 -0.359815   \n",
       "10276  0.357298 -0.044599 -0.129532 -0.314497 -0.055580 -0.173090 -0.385964   \n",
       "10277  0.344328  0.004793 -0.122453 -0.319987 -0.066677 -0.181803 -0.380404   \n",
       "10278  0.283644 -0.007958 -0.119018 -0.308558 -0.080392 -0.211476 -0.368919   \n",
       "10279  0.206988  0.024602 -0.103940 -0.364650 -0.169378 -0.215814 -0.449295   \n",
       "10280  0.392804 -0.017788 -0.090166 -0.096349 -0.174368 -0.256686 -0.152987   \n",
       "10281  0.301162 -0.030034 -0.120333 -0.056181 -0.151753 -0.254848 -0.093334   \n",
       "10282  0.288215  0.011493 -0.155453 -0.063550 -0.035160 -0.230059 -0.093119   \n",
       "10283  0.114866 -0.000427 -0.116904 -0.189544  0.078012 -0.312489 -0.268158   \n",
       "10284  0.126970 -0.013276 -0.073441 -0.173816  0.041540 -0.342513 -0.253156   \n",
       "10285  0.363943 -0.022212 -0.125437 -0.041180  0.020777 -0.243607 -0.113519   \n",
       "10286  0.331044 -0.063979 -0.117333 -0.068002  0.156431 -0.317060 -0.148922   \n",
       "10287  0.272259 -0.007579 -0.072642 -0.072559  0.005472 -0.225400 -0.161118   \n",
       "10288  0.277271  0.001084 -0.074001 -0.068523 -0.245059 -0.144552 -0.148731   \n",
       "10289  0.305631 -0.052702 -0.155684 -0.343078 -0.151250 -0.177296 -0.383228   \n",
       "10290  0.321443 -0.023799 -0.119695 -0.336548 -0.183307 -0.154297 -0.396342   \n",
       "10291  0.267413 -0.021596 -0.070550 -0.323426 -0.118042 -0.323907 -0.395468   \n",
       "10292  0.147146 -0.046908 -0.069338 -0.348686  0.018057 -0.364716 -0.437199   \n",
       "10293  0.192275 -0.033643 -0.105949 -0.354841 -0.092504 -0.312910 -0.433579   \n",
       "10294  0.310155 -0.053391 -0.099109 -0.287866 -0.140589 -0.215088 -0.356083   \n",
       "10295  0.363385 -0.039214 -0.105915 -0.305388  0.028148 -0.196373 -0.373540   \n",
       "10296  0.349966  0.030077 -0.115788 -0.329638 -0.042143 -0.250181 -0.388017   \n",
       "10297  0.237594  0.018467 -0.096499 -0.323114 -0.229775 -0.207574 -0.392380   \n",
       "10298  0.153627 -0.018437 -0.137018 -0.330046 -0.195253 -0.164339 -0.430974   \n",
       "\n",
       "             V8        V9       V10    ...         V552      V553      V554  \\\n",
       "0     -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
       "1     -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
       "2     -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
       "3     -0.982750 -0.989303 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
       "4     -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
       "5     -0.990218 -0.995549 -0.942469    ...     0.019953 -0.545410 -0.844619   \n",
       "6     -0.966097 -0.983116 -0.940987    ...     0.145844 -0.217198 -0.564430   \n",
       "7     -0.966313 -0.982982 -0.940987    ...     0.136382 -0.082307 -0.421715   \n",
       "8     -0.957236 -0.984379 -0.940598    ...     0.314038 -0.269401 -0.572995   \n",
       "9     -0.973663 -0.985642 -0.940028    ...     0.267383  0.339526  0.140452   \n",
       "10    -0.973948 -0.985172 -0.940028    ...     0.120503  0.348771  0.057682   \n",
       "11    -0.985997 -0.993182 -0.943906    ...     0.351442 -0.611014 -0.878363   \n",
       "12    -0.982937 -0.996410 -0.943906    ...     0.689897 -0.686389 -0.878751   \n",
       "13    -0.990687 -0.992168 -0.943323    ...     0.740023 -0.564100 -0.765901   \n",
       "14    -0.794280 -0.888015 -0.925977    ...     0.130958  0.207689 -0.068054   \n",
       "15    -0.973332 -0.984065 -0.941716    ...     0.661540 -0.782137 -0.953520   \n",
       "16    -0.981620 -0.981336 -0.941716    ...     0.560668 -0.778877 -0.940421   \n",
       "17    -0.987954 -0.992190 -0.942076    ...     0.428614 -0.328899 -0.596861   \n",
       "18    -0.992711 -0.996492 -0.944870    ...     0.348413 -0.501301 -0.838243   \n",
       "19    -0.989470 -0.997193 -0.945662    ...     0.667156 -0.941671 -0.965522   \n",
       "20    -0.993604 -0.995595 -0.941472    ...     0.601134 -0.833890 -0.968580   \n",
       "21    -0.989669 -0.996700 -0.941472    ...     0.576339 -0.848198 -0.950247   \n",
       "22    -0.989473 -0.994485 -0.944567    ...     0.480225 -0.701970 -0.895118   \n",
       "23    -0.986771 -0.989637 -0.943675    ...     0.468191 -0.909786 -0.986595   \n",
       "24    -0.988544 -0.993287 -0.942559    ...     0.730084 -0.746382 -0.941664   \n",
       "25    -0.985192 -0.995018 -0.942559    ...     0.677073 -0.715462 -0.937483   \n",
       "26    -0.993620 -0.993583 -0.942890    ...     0.718284 -0.824919 -0.964407   \n",
       "27    -0.110813 -0.400599 -0.931896    ...    -0.257607  0.156185 -0.241781   \n",
       "28    -0.891373 -0.933361 -0.931896    ...     0.582426 -0.743397 -0.899523   \n",
       "29    -0.936927 -0.980669 -0.935389    ...     0.755593 -0.768623 -0.928548   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "10269 -0.180600 -0.265530 -0.072573    ...    -0.191368 -0.323285 -0.752765   \n",
       "10270 -0.238855 -0.235164 -0.149166    ...    -0.043261 -0.221525 -0.650547   \n",
       "10271 -0.226048 -0.220538  0.092017    ...    -0.178355 -0.141846 -0.564142   \n",
       "10272 -0.130102 -0.322741  0.092017    ...    -0.119239 -0.297224 -0.721821   \n",
       "10273 -0.036371 -0.327340 -0.017856    ...    -0.143979 -0.374529 -0.779528   \n",
       "10274 -0.087048 -0.256664  0.056244    ...    -0.030962 -0.138929 -0.589311   \n",
       "10275 -0.136375 -0.290923  0.056244    ...     0.009556 -0.291001 -0.704020   \n",
       "10276 -0.057525 -0.216827  0.026249    ...     0.016849 -0.162555 -0.593285   \n",
       "10277 -0.071025 -0.244559 -0.116867    ...    -0.029216  0.181042 -0.250170   \n",
       "10278 -0.097063 -0.301231 -0.116867    ...    -0.110489  0.024468 -0.392944   \n",
       "10279 -0.185957 -0.326199 -0.175652    ...    -0.214243 -0.351854 -0.734494   \n",
       "10280 -0.207809 -0.265253  0.485680    ...     0.089373  0.274132 -0.036768   \n",
       "10281 -0.206796 -0.275332  0.485680    ...    -0.074873  0.430681  0.204816   \n",
       "10282 -0.116987 -0.258696  0.160374    ...    -0.112290  0.055902 -0.299539   \n",
       "10283 -0.047632 -0.341972  0.160374    ...    -0.120152 -0.148767 -0.564841   \n",
       "10284 -0.093578 -0.350155  0.285773    ...     0.199365  0.048103 -0.252670   \n",
       "10285 -0.120400 -0.221709  0.411506    ...     0.155538  0.118344 -0.205856   \n",
       "10286  0.070107 -0.290868  0.411506    ...    -0.021385 -0.086279 -0.468001   \n",
       "10287 -0.074187 -0.280956  0.402615    ...    -0.026374 -0.104077 -0.408741   \n",
       "10288 -0.303187 -0.198912  0.402615    ...    -0.103873  0.160861 -0.125531   \n",
       "10289 -0.181989 -0.287325 -0.108216    ...    -0.032052 -0.285985 -0.714939   \n",
       "10290 -0.224409 -0.220786 -0.073510    ...     0.005331 -0.172403 -0.563389   \n",
       "10291 -0.129623 -0.335741  0.053516    ...     0.215341 -0.388926 -0.761280   \n",
       "10292  0.019297 -0.411240  0.053516    ...     0.166456 -0.552567 -0.850109   \n",
       "10293 -0.088741 -0.336060 -0.041624    ...     0.158728 -0.629657 -0.916493   \n",
       "10294 -0.148775 -0.232057  0.185361    ...     0.074472 -0.376278 -0.750809   \n",
       "10295 -0.030036 -0.270237  0.185361    ...     0.101859 -0.320418 -0.700274   \n",
       "10296 -0.133257 -0.347029  0.007471    ...    -0.066249 -0.118854 -0.467179   \n",
       "10297 -0.279610 -0.289477  0.007471    ...    -0.046467 -0.205445 -0.617737   \n",
       "10298 -0.218295 -0.229933 -0.111527    ...    -0.010386 -0.072237 -0.436940   \n",
       "\n",
       "           V555      V556      V557      V558      V559      V560      V561  \n",
       "0     -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1      0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2     -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3     -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4      0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "5      0.082632 -0.143439  0.275041 -0.368224 -0.849632  0.184823 -0.042126  \n",
       "6     -0.212754 -0.230622  0.014637 -0.189512 -0.852150  0.182170 -0.043010  \n",
       "7     -0.020888  0.593996 -0.561871  0.467383 -0.851017  0.183779 -0.041976  \n",
       "8      0.012954  0.080936 -0.234313  0.117797 -0.847971  0.188982 -0.037364  \n",
       "9     -0.020590 -0.127730 -0.482871 -0.070670 -0.848294  0.190310 -0.034417  \n",
       "10     0.080699  0.595791 -0.475802  0.115931 -0.851562  0.187609 -0.034681  \n",
       "11     0.001761 -0.065980  0.578861 -0.651945 -0.852723  0.186050 -0.035852  \n",
       "12    -0.077552 -0.101222  0.639084  0.765485 -0.850654  0.187611 -0.035998  \n",
       "13     0.105620 -0.090278 -0.132403  0.498814 -0.849773  0.188812 -0.035063  \n",
       "14     0.062297 -0.058719  0.031208 -0.268791 -0.730937  0.283159  0.036444  \n",
       "15    -0.121852 -0.029077 -0.013034 -0.056927 -0.761101  0.263119  0.024172  \n",
       "16    -0.001446 -0.048110 -0.340473 -0.229155 -0.759172  0.264324  0.027014  \n",
       "17    -0.028332  0.092367 -0.822239  0.367557 -0.759363  0.264033  0.029664  \n",
       "18    -0.165849 -0.033007 -0.240572  0.788193 -0.761052  0.262886  0.029346  \n",
       "19     0.244931  0.102569  0.066135 -0.411729 -0.760620  0.263169  0.029573  \n",
       "20     0.160607  0.197774  0.257657 -0.381102 -0.760528  0.263183  0.030288  \n",
       "21    -0.002320  0.150391  0.142331 -0.853711 -0.762023  0.262170  0.029987  \n",
       "22    -0.032337 -0.301298  0.132576 -0.022379 -0.761509  0.262550  0.029639  \n",
       "23    -0.542884 -0.249545  0.006986 -0.235200 -0.758960  0.264256  0.030456  \n",
       "24    -0.021446  0.337010 -0.436685 -0.622922 -0.758977  0.264224  0.030743  \n",
       "25     0.025652  0.066503 -0.226316 -0.225358 -0.762197  0.262090  0.029404  \n",
       "26     0.231060  0.429283  0.681154  0.815226 -0.763702  0.261103  0.028563  \n",
       "27     0.013526  0.043354  0.021485  0.046689 -0.667085  0.054216 -0.218875  \n",
       "28     0.194735 -0.148056  0.033529 -0.127028 -0.564807 -0.027045 -0.266055  \n",
       "29    -0.228688 -0.097215  0.024192  0.006329 -0.579367 -0.021567 -0.257530  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10269 -0.829318  0.048275  0.913474 -0.903792 -0.694649  0.245906  0.172715  \n",
       "10270 -0.241364 -0.181262 -0.946507  0.033174 -0.722767  0.244779  0.144695  \n",
       "10271  0.004509  0.357282 -0.945578  0.613976 -0.694613  0.259140  0.157683  \n",
       "10272  0.752448 -0.835923 -0.918626  0.109159 -0.671194  0.265751  0.174766  \n",
       "10273  0.556469 -0.206396 -0.943786  0.297510 -0.661045  0.270618  0.179726  \n",
       "10274  0.273411  0.855750 -0.962498  0.953145 -0.657085  0.276184  0.177337  \n",
       "10275 -0.300646  0.225721  0.868469 -0.461542 -0.663506  0.273305  0.173953  \n",
       "10276 -0.710861 -0.061232 -0.706116  0.064574 -0.660163  0.274327  0.176291  \n",
       "10277 -0.402779 -0.706228  0.738571  0.870613 -0.652636  0.277868  0.180047  \n",
       "10278 -0.076085 -0.238582  0.960357  0.086598 -0.656760  0.271648  0.182987  \n",
       "10279  0.535018 -0.256868  0.927325 -0.084328 -0.657011  0.266990  0.187901  \n",
       "10280 -0.742630 -0.080227  0.927331 -0.652179 -0.807271  0.189885  0.118456  \n",
       "10281 -0.464962  0.291873  0.921466 -0.869951 -0.814760  0.190418  0.110325  \n",
       "10282 -0.057336 -0.592802  0.964828  0.606231 -0.809277  0.192232  0.113997  \n",
       "10283  0.849907 -0.643277  0.973227  0.062755 -0.809257  0.185954  0.120358  \n",
       "10284  0.872466  0.491872 -0.155724 -0.762511 -0.800932  0.190968  0.123600  \n",
       "10285 -0.895624  0.648047 -0.624180 -0.441569 -0.803888  0.191732  0.119944  \n",
       "10286 -0.351287 -0.335934  0.966914 -0.715323 -0.810091  0.184712  0.120724  \n",
       "10287  0.181631  0.491898 -0.977969 -0.124243 -0.797719  0.194819  0.122937  \n",
       "10288  0.133749  0.882511 -0.994293  0.475236 -0.803914  0.196823  0.114498  \n",
       "10289 -0.338596  0.363565 -0.951163 -0.228412 -0.691122  0.241953  0.180141  \n",
       "10290 -0.874477 -0.684506 -0.948809  0.472612 -0.677946  0.256877  0.177768  \n",
       "10291  0.218079 -0.690839 -0.922779  0.232523 -0.672635  0.261034  0.178609  \n",
       "10292  0.524082  0.041970 -0.922941  0.489178 -0.660366  0.272243  0.178547  \n",
       "10293  0.535983  0.689306 -0.936606  0.562375 -0.646754  0.282150  0.181152  \n",
       "10294 -0.337422  0.346295  0.884904 -0.698885 -0.651732  0.274627  0.184784  \n",
       "10295 -0.736701 -0.372889 -0.657421  0.322549 -0.655181  0.273578  0.182412  \n",
       "10296 -0.181560  0.088574  0.696664  0.363139 -0.655357  0.274479  0.181184  \n",
       "10297  0.444558 -0.819188  0.929294 -0.008398 -0.659719  0.264782  0.187563  \n",
       "10298  0.598808 -0.287951  0.876030 -0.024965 -0.660080  0.263936  0.188103  \n",
       "\n",
       "[10299 rows x 561 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7209, 561)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3090, 561)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7209,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3090,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 SVM (Support Vector Machine) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, average_precision_score\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training a linear SVM classifier\n",
    "\n",
    "svm_lm = SVC().fit(X_train, y_train)\n",
    "y_svm_pred = svm_lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on test set:0.9498\n"
     ]
    }
   ],
   "source": [
    "# model accuracy for X_test  \n",
    "\n",
    "accuracy = svm_lm.score(X_test, y_test)\n",
    "print('Accuracy of SVM classifier on test set:{:.4f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[533   6   0   0   0   0]\n",
      " [  2 416   6   0   0   0]\n",
      " [  5  16 399   0   0   0]\n",
      " [  0   3   0 496  61   3]\n",
      " [  0   0   0  53 494   0]\n",
      " [  0   0   0   0   0 597]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_svm_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.99      0.99       539\n",
      "          2       0.94      0.98      0.96       424\n",
      "          3       0.99      0.95      0.97       420\n",
      "          4       0.90      0.88      0.89       563\n",
      "          5       0.89      0.90      0.90       547\n",
      "          6       0.99      1.00      1.00       597\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3090\n",
      "\n",
      "Accuracy Score for SVM Classifier: 0.949838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_svm_pred))\n",
    "print('Accuracy Score for SVM Classifier: %2f'% accuracy_score(y_test, y_svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Hyperparameter Tuning for SVC using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=linear, score=0.9804573804573805, total=   3.7s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.1, kernel=linear, score=0.9800249687890137, total=   4.8s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   16.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.1, kernel=linear, score=0.9791753436068305, total=   4.4s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   24.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.8819126819126819, total=  29.5s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.8847274240532668, total=  35.4s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.8871303623490212, total=  32.9s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=linear, score=0.9804573804573805, total=   4.1s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=linear, score=0.9800249687890137, total=   4.1s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=linear, score=0.9791753436068305, total=   4.1s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.9151767151767152, total=  16.4s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.9113607990012484, total=  18.4s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.9196168263223656, total=  18.6s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=linear, score=0.9804573804573805, total=   3.8s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=linear, score=0.9800249687890137, total=   3.2s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=linear, score=0.9791753436068305, total=   3.5s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.7617463617463618, total=  33.8s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.7673741156887224, total=  31.5s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.7580174927113703, total=  29.3s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=linear, score=0.9804573804573805, total=   5.7s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=linear, score=0.9800249687890137, total=   4.6s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=linear, score=0.9791753436068305, total=   3.8s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.3525987525987526, total=  44.7s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.3524760715771952, total=  44.2s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.35276967930029157, total=  49.8s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV]  C=1, gamma=0.1, kernel=linear, score=0.9825363825363825, total=   3.6s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV]  C=1, gamma=0.1, kernel=linear, score=0.9850187265917603, total=   3.8s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV]  C=1, gamma=0.1, kernel=linear, score=0.9820907955018742, total=   4.2s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.9792099792099792, total=  24.3s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.9725343320848939, total=  20.2s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.9800083298625573, total=  20.2s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV]  C=1, gamma=0.01, kernel=linear, score=0.9825363825363825, total=   3.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV]  C=1, gamma=0.01, kernel=linear, score=0.9850187265917603, total=   2.8s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV]  C=1, gamma=0.01, kernel=linear, score=0.9820907955018742, total=   2.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.9738045738045739, total=   6.9s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.9704535996670828, total=   7.1s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.9725114535610162, total=   7.6s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV]  C=1, gamma=0.001, kernel=linear, score=0.9825363825363825, total=   2.8s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV]  C=1, gamma=0.001, kernel=linear, score=0.9850187265917603, total=   2.7s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV]  C=1, gamma=0.001, kernel=linear, score=0.9820907955018742, total=   3.2s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9214137214137215, total=  12.1s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9230129005409904, total=  11.7s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9262807163681799, total=  12.1s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=linear, score=0.9825363825363825, total=   2.8s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=linear, score=0.9850187265917603, total=   2.9s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=linear, score=0.9820907955018742, total=   4.7s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.7625779625779626, total=  29.2s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.7694548481065335, total=  29.7s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.760932944606414, total=  32.1s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV]  C=10, gamma=0.1, kernel=linear, score=0.9808731808731809, total=   3.1s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV]  C=10, gamma=0.1, kernel=linear, score=0.9837702871410736, total=   3.2s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV]  C=10, gamma=0.1, kernel=linear, score=0.9820907955018742, total=   3.8s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.9808731808731809, total=  28.5s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.9750312109862672, total=  26.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.9837567680133278, total=  27.2s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV]  C=10, gamma=0.01, kernel=linear, score=0.9808731808731809, total=   3.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV]  C=10, gamma=0.01, kernel=linear, score=0.9837702871410736, total=   3.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV]  C=10, gamma=0.01, kernel=linear, score=0.9820907955018742, total=   3.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.987941787941788, total=   4.8s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.9879317519766958, total=   4.7s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.9866722199083715, total=   4.9s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV]  C=10, gamma=0.001, kernel=linear, score=0.9808731808731809, total=   3.0s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV]  C=10, gamma=0.001, kernel=linear, score=0.9837702871410736, total=   3.0s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV]  C=10, gamma=0.001, kernel=linear, score=0.9820907955018742, total=   3.2s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9721413721413722, total=   7.2s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9704535996670828, total=   5.7s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9725114535610162, total=   5.1s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV]  C=10, gamma=0.0001, kernel=linear, score=0.9808731808731809, total=   3.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV]  C=10, gamma=0.0001, kernel=linear, score=0.9837702871410736, total=   3.1s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV]  C=10, gamma=0.0001, kernel=linear, score=0.9820907955018742, total=   3.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9218295218295218, total=  12.7s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9234290470245526, total=  11.5s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9266972094960433, total=  11.8s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV]  C=100, gamma=0.1, kernel=linear, score=0.9787941787941788, total=   2.8s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV]  C=100, gamma=0.1, kernel=linear, score=0.9821057012068248, total=   2.8s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV]  C=100, gamma=0.1, kernel=linear, score=0.9808413161182841, total=   3.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.9808731808731809, total=  21.9s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.9750312109862672, total=  22.8s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.9837567680133278, total=  21.4s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=100, gamma=0.01, kernel=linear, score=0.9787941787941788, total=   2.8s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=100, gamma=0.01, kernel=linear, score=0.9821057012068248, total=   2.9s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV]  C=100, gamma=0.01, kernel=linear, score=0.9808413161182841, total=   2.9s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.9875259875259875, total=   4.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.9900124843945068, total=   3.8s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.9895876718034152, total=   4.1s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=100, gamma=0.001, kernel=linear, score=0.9787941787941788, total=   2.8s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=100, gamma=0.001, kernel=linear, score=0.9821057012068248, total=   2.7s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV]  C=100, gamma=0.001, kernel=linear, score=0.9808413161182841, total=   2.8s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9841995841995842, total=   3.3s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9829379941739492, total=   3.5s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9808413161182841, total=   3.4s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=100, gamma=0.0001, kernel=linear, score=0.9787941787941788, total=   2.7s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=100, gamma=0.0001, kernel=linear, score=0.9821057012068248, total=   2.8s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV]  C=100, gamma=0.0001, kernel=linear, score=0.9808413161182841, total=   2.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9704781704781705, total=   5.5s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9704535996670828, total=   6.7s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9712619741774261, total=   5.2s\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=linear, score=0.9787941787941788, total=   2.7s\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=linear, score=0.9821057012068248, total=   3.1s\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=linear, score=0.9808413161182841, total=   3.1s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.9808731808731809, total=  22.3s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.9750312109862672, total=  22.2s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.9837567680133278, total=  22.3s\n",
      "[CV] C=1000, gamma=0.01, kernel=linear ...............................\n",
      "[CV]  C=1000, gamma=0.01, kernel=linear, score=0.9787941787941788, total=   4.1s\n",
      "[CV] C=1000, gamma=0.01, kernel=linear ...............................\n",
      "[CV]  C=1000, gamma=0.01, kernel=linear, score=0.9821057012068248, total=   3.5s\n",
      "[CV] C=1000, gamma=0.01, kernel=linear ...............................\n",
      "[CV]  C=1000, gamma=0.01, kernel=linear, score=0.9808413161182841, total=   2.8s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.9875259875259875, total=   4.2s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.9904286308780691, total=   3.9s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.9891711786755518, total=   3.9s\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV]  C=1000, gamma=0.001, kernel=linear, score=0.9787941787941788, total=   2.7s\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV]  C=1000, gamma=0.001, kernel=linear, score=0.9821057012068248, total=   2.6s\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV]  C=1000, gamma=0.001, kernel=linear, score=0.9808413161182841, total=   2.7s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9854469854469855, total=   3.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9862671660424469, total=   3.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9841732611411912, total=   2.9s\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1000, gamma=0.0001, kernel=linear, score=0.9787941787941788, total=   2.7s\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=linear, score=0.9821057012068248, total=   2.8s\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=linear, score=0.9808413161182841, total=   2.7s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.982952182952183, total=   3.1s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9829379941739492, total=   3.4s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9795918367346939, total=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 32.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set found:\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Detailed grid scores:\n",
      "0.980 (+/-0.001) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.885 (+/-0.004) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.980 (+/-0.001) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "0.915 (+/-0.007) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.980 (+/-0.001) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "0.762 (+/-0.008) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.980 (+/-0.001) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "0.353 (+/-0.000) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "0.983 (+/-0.003) for {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.977 (+/-0.007) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.983 (+/-0.003) for {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "0.972 (+/-0.003) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.983 (+/-0.003) for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "0.924 (+/-0.004) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.983 (+/-0.003) for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "0.764 (+/-0.007) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "0.982 (+/-0.002) for {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.980 (+/-0.007) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.982 (+/-0.002) for {'C': 10, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "0.988 (+/-0.001) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.982 (+/-0.002) for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "0.972 (+/-0.002) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.982 (+/-0.002) for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "0.924 (+/-0.004) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.980 (+/-0.007) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 100, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "0.989 (+/-0.002) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "0.983 (+/-0.003) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "0.971 (+/-0.001) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.980 (+/-0.007) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 1000, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "\n",
      "0.989 (+/-0.002) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "0.985 (+/-0.002) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.981 (+/-0.003) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "0.982 (+/-0.003) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "              \"kernel\" : ['linear', 'rbf'],       \n",
    "               \"gamma\" : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                   \"C\" : [0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "selector = GridSearchCV(SVC(), parameters, scoring='accuracy', verbose=4) \n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set found:')\n",
    "print(selector.best_params_)\n",
    "print('Detailed grid scores:')\n",
    "means = selector.cv_results_['mean_test_score']\n",
    "stds = selector.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, selector.cv_results_['params']):\n",
    "    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVM Classifier after hyperparameter tuning:0.988026\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=100, gamma=0.01).fit(X_train, y_train)\n",
    "y_pred_best_svm = clf.predict(X_test)\n",
    "print('Accuracy score for SVM Classifier after hyperparameter tuning:%2f'% accuracy_score(y_test, y_pred_best_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[539   0   0   0   0   0]\n",
      " [  0 422   2   0   0   0]\n",
      " [  0   0 420   0   0   0]\n",
      " [  0   3   0 543  17   0]\n",
      " [  0   0   0  15 532   0]\n",
      " [  0   0   0   0   0 597]]\n"
     ]
    }
   ],
   "source": [
    "#CONFUSION MATRIX\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred_best_svm)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       539\n",
      "          2       0.99      1.00      0.99       424\n",
      "          3       1.00      1.00      1.00       420\n",
      "          4       0.97      0.96      0.97       563\n",
      "          5       0.97      0.97      0.97       547\n",
      "          6       1.00      1.00      1.00       597\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3090\n",
      "\n",
      "Accuracy for Logistic Regression After Hyperparameter Tuning: 0.988026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_best_svm))\n",
    "print('Accuracy for Logistic Regression After Hyperparameter Tuning: %2f'% accuracy_score(y_test, y_pred_best_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.98839, R: 0.98872\n"
     ]
    }
   ],
   "source": [
    "prec = precision_score(y_test, y_pred_best_svm, average=\"macro\")\n",
    "rec = recall_score(y_test, y_pred_best_svm, average=\"macro\")\n",
    "print(\"P: %3.5f, R: %3.5f\" % (prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/7S+zdYc9xMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106274c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"Learning Curves (SVM)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "\n",
    "estimator = clf\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
